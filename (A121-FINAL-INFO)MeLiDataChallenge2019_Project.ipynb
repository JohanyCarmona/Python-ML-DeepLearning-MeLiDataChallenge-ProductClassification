{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AK5dPQ-9g07M"
   },
   "source": [
    "**Necessary libraries and paths**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q4BADFqqfhyf"
   },
   "outputs": [],
   "source": [
    "from fastai.text import * \n",
    "from fastai.callbacks import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YN-ErkaIga2I"
   },
   "outputs": [],
   "source": [
    "#This ever will be 'True\n",
    "SAMPLE = True\n",
    "PATH = Path('./sample') if SAMPLE else Path('.')\n",
    "PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7vEH88O-gcsV"
   },
   "outputs": [],
   "source": [
    "MODELS_PATH = PATH / 'models'\n",
    "MODELS_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rb2HM9GNyUpt"
   },
   "source": [
    "**DOWNLOAD THE DATA**\n",
    "> Now we going to connect to Google Drive to download the project files\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L8_kVlwD3MJo"
   },
   "outputs": [],
   "source": [
    "if not (PATH / 'train.csv').exists() or not (PATH / 'test.csv').exists():\n",
    "  !pip install PyDrive\n",
    "  import os\n",
    "  from pydrive.auth import GoogleAuth\n",
    "  from pydrive.drive import GoogleDrive\n",
    "  from google.colab import auth\n",
    "  from oauth2client.client import GoogleCredentials\n",
    "  \n",
    "  auth.authenticate_user()\n",
    "  gauth = GoogleAuth()\n",
    "  gauth.credentials = GoogleCredentials.get_application_default()\n",
    "  drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s6qT3kpv344o"
   },
   "source": [
    "We going to download now the Train and Test Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8yncuiR74CWu"
   },
   "outputs": [],
   "source": [
    "if not (PATH / 'train.csv').exists():\n",
    "  #The 'id' of your file share using advanced sharing for collaborators\n",
    "  TrainData = drive.CreateFile({'id': '1k4N_Ia7Z_5QfZWg16AOmBMk89_Szhskl'})\n",
    "  #The 'name of your file' that you uploaded in the 'sample' folder.\n",
    "  TrainData.GetContentFile(PATH / 'train.csv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lazEirL84cRa"
   },
   "outputs": [],
   "source": [
    "if not (PATH / 'test.csv').exists():\n",
    "  TestData = drive.CreateFile({'id': '105vlQHX9-mveztq6wHR9Ekfm7Yfq7T4F'})\n",
    "  TestData.GetContentFile(PATH / 'test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dgf31V1J6FgE"
   },
   "source": [
    "The train.csv.gz file is going to extract right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ygJcYMct59a1"
   },
   "outputs": [],
   "source": [
    "if not (PATH / 'train.csv').exists():\n",
    "  PATH_TRAIN_COMP = PATH / 'train.csv.gz'\n",
    "  !gzip -d $PATH_TRAIN_COMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tl-N0-ic67gV"
   },
   "source": [
    "**Data Visualization and Pre-processing**\n",
    "\n",
    "\n",
    "> Now, we are going to delete all of characters what contain accents, uppercase words, &c.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XxYVtvS5sLFu"
   },
   "outputs": [],
   "source": [
    "Asuffix = 'A-ret_cat_'\n",
    "def return_categories(PATH = PATH, filename = 'train.csv', save = True, old = True, suffix = Asuffix):\n",
    "  list_categories=list()\n",
    "  if old == True and (PATH / (suffix + filename)).exists():\n",
    "      data_file = open(PATH / ( suffix + filename), 'r')\n",
    "      text_file=data_file.read()\n",
    "      data_file.close()  \n",
    "      list_categories=text_file.split(' ')\n",
    "\n",
    "  if old == False or not(PATH / (suffix + filename)).exists():\n",
    "    df = pd.read_csv(PATH / filename)\n",
    "    title_category=df['category'].unique()\n",
    "    for i in range(len(title_category)-1):\n",
    "      list_categories.append(title_category[i])\n",
    "    list_categories.append(title_category[-1])\n",
    "    list_categories.sort()\n",
    "    if save == True:\n",
    "      text_file=''\n",
    "      for i in range(len(list_categories)-1):\n",
    "        text_file=text_file+list_categories[i]+' '\n",
    "      text_file=text_file+list_categories[-1]\n",
    "      data_file = open(PATH / (suffix + filename), 'w')\n",
    "      data_file.write(text_file)\n",
    "      data_file.close()\n",
    "  return list_categories\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are obtaining the total of categories in all of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Oq3oqB3ulNEI"
   },
   "outputs": [],
   "source": [
    "AAsuffix = 'AA-ret_num_cat_' \n",
    "def return_number_categories(PATH = PATH, filename = 'train.csv', save = True, old = True, suffix = AAsuffix):#'./sample', filename = 'train.csv', save = True, old = True, suffix = AAsuffix):\n",
    "  if old == True and (PATH / (suffix + filename)).exists():\n",
    "    file = open(PATH / (suffix + filename),'r')\n",
    "    data=file.read()\n",
    "    file.close()\n",
    "    number_categories = int(data)\n",
    "  if old == False or not(PATH / (suffix + filename)).exists():\n",
    "    list_categories = return_categories(PATH, filename, save, old)\n",
    "    number_categories = len(list_categories)\n",
    "    if save == True:\n",
    "      data = str(number_categories)\n",
    "      file = open(PATH / (suffix + filename), 'w')\n",
    "      file.write(data)\n",
    "      file.close()\n",
    "  return number_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GDfW_0sgwMdW"
   },
   "source": [
    "***Parameters***\n",
    "\n",
    "> *Setting the parameters*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we the split the data in both languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bsuffix = 'B-spl_lan_'\n",
    "def split_language(PATH, filename, language, save = True, old = True, suffix = Bsuffix): # language = 'spanish' or 'portuguese'\n",
    "  suffix = suffix + language[:3] + '_'\n",
    "  if old == True and (PATH / (suffix + filename)).exists():\n",
    "      df = pd.read_csv(PATH / (suffix + filename))\n",
    "  if old == False or not(PATH / (suffix + filename)).exists():\n",
    "    df = pd.read_csv(PATH / filename)\n",
    "    df = df[df['language'] == language]\n",
    "    if save == True:\n",
    "      df.to_csv(PATH / (suffix + filename), index=False)\n",
    "  return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_quantities(PATH, filename, number_categories):\n",
    "  list_categories_language = return_categories(PATH, filename, save = False, old = False)\n",
    "  quantity_categories = np.zeros(number_categories)\n",
    "  df = pd.read_csv(PATH / filename)\n",
    "  df = df.groupby(['category']).count().title\n",
    "  for i in range(len(df)):\n",
    "    quantity_categories[list_categories.index(list_categories_language[i])] = df[i]\n",
    "  return quantity_categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_fQ2Z-la43gD"
   },
   "outputs": [],
   "source": [
    "Csuffix = 'C-spl_set_'\n",
    "def split_set(PATH, filename, pi, seed, return_train = True, old = True, suffix = Csuffix):\n",
    "  if old == True and (PATH / (suffix + 'train_' + filename)).exists() and (PATH / (suffix + 'test_' + filename)).exists():\n",
    "      df_train = pd.read_csv(PATH / (suffix + 'train_' + filename))\n",
    "      df_test = pd.read_csv(PATH / (suffix + 'test_' + filename))\n",
    "  if old == False or not((PATH / (suffix + 'train_' + filename)).exists() and (PATH / (suffix + 'test_' + filename)).exists()):\n",
    "    df = pd.read_csv(PATH / filename)\n",
    "    df_test, df_train =  train_test_split(df, test_size=int(pi*len(df)), random_state=seed, stratify=df.category)\n",
    "    df_train.to_csv(PATH / (suffix + 'train_' + filename), index=False)\n",
    "    df_test.to_csv(PATH / (suffix + 'test_' + filename), index=False)\n",
    "  if return_train == True:\n",
    "    return df_train\n",
    "  if return_train == False:\n",
    "    return df_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function returns the total of elements with the quality 'reliable' for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MhvIaBRYJ8JZ"
   },
   "outputs": [],
   "source": [
    "def return_qualities(PATH, filename, number_categories, quantity_categories):\n",
    "  quality_categories_reliable = return_quantities(PATH, filename, number_categories)\n",
    "  quality_categories = np.zeros(number_categories)\n",
    "  quality_categories_rel = np.zeros(number_categories)\n",
    "  quality_categories_unr = np.zeros(number_categories)\n",
    "  for i in range(number_categories):\n",
    "    quality_categories[i] = quantity_categories[i]\n",
    "    quality_categories_rel[i] = quality_categories_reliable[i]\n",
    "    quality_categories_unr[i] = quantity_categories[i] - quality_categories_reliable[i]\n",
    "  return quality_categories, quality_categories_rel, quality_categories_unr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function save a new data with the language choosed for the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Dsuffix = 'D-spl_lab_qua_'\n",
    "def split_label_quality(PATH, filename, label_quality, save = False, old = True, suffix = Dsuffix): # label_quality = 'reliable' or 'unreliable'\n",
    "  suffix = suffix + label_quality[:3] + '_'\n",
    "  if old == True and (PATH / (suffix + filename)).exists():\n",
    "    df = pd.read_csv(PATH / (suffix + filename))\n",
    "  if old == False or not(PATH / (suffix + filename)).exists():\n",
    "    df = pd.read_csv(PATH / filename)\n",
    "    df = df[df['label_quality'] == label_quality]\n",
    "    if save == True:\n",
    "      df.to_csv(PATH / (suffix + filename), index=False)\n",
    "  return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This a constant created to found the number of elements necessary for extract the mount of elements type 'reliable' that the user choosed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xkHDjt8OJ-9J"
   },
   "outputs": [],
   "source": [
    "def return_gamma(num_rel, num_D, alpha, pi, tetha = 0.0):\n",
    "  proportion_rel = num_rel / num_D\n",
    "  alpha_dec = float(\"{0:.2f}\".format(alpha))\n",
    "  proportion_rel_dec = float(\"{0:.2f}\".format(proportion_rel))\n",
    "  if alpha_dec > proportion_rel_dec:\n",
    "    gamma = (alpha / proportion_rel)*(1 + tetha) - tetha\n",
    "  if alpha_dec == proportion_rel_dec:\n",
    "    gamma = 1 + (1 / (pi*num_D) )\n",
    "  if alpha_dec < proportion_rel_dec:\n",
    "    gamma = ((1 - alpha) / (1 - proportion_rel))*(1 + tetha) - tetha\n",
    "  return gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the final function to split the data with the language that the user wanted extract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Esuffix = 'E-spl_dat_'\n",
    "#(!)# filename = 'train.csv'\n",
    "def split_data(PATH, filename, language, pi, alpha, seed, save = True, old = True, suffix = Esuffix):\n",
    "  tetha = 0.00 # Parameter for adjust the parameter gamma to prevening the error in the function.\n",
    "  if old == True and (PATH / (suffix + language[:3] + '_' + filename)).exists():\n",
    "    df = pd.read_csv(PATH / (suffix + language[:3] + '_' + filename))\n",
    "  if old == False or not(PATH / (suffix + language[:3] + '_' + filename)).exists():\n",
    "    df = split_language(PATH, filename, language, old = True) #old = old)\n",
    "    num_D = len(df)\n",
    "    df_rel = split_label_quality(PATH, Bsuffix + language[:3] + '_' + filename, 'reliable', save = False, old = old)\n",
    "    num_rel = len(df_rel)\n",
    "    gamma = return_gamma(num_rel, num_D, alpha, pi, tetha = 0.00)\n",
    "    df = split_set(PATH, (Bsuffix + language[:3] + '_' + filename), gamma*pi, seed, return_train = True, old = old)\n",
    "    num_D_train = len(df)\n",
    "    df_rel = split_label_quality(PATH, Csuffix + 'train_' + Bsuffix + language[:3] + '_' + filename, 'reliable', save = False, old = old)\n",
    "    num_rel_train = len(df_rel)\n",
    "    df_unr = split_label_quality(PATH, Csuffix + 'train_' + Bsuffix + language[:3] + '_' + filename, 'unreliable', save = False, old = old)\n",
    "    num_unr_train = len(df_unr)\n",
    "    df_residual, df_unr = train_test_split(df_unr, test_size=int(num_D*pi-num_rel_train), random_state = seed) # The line: #, stratify=df_unr.category) was deleted because it generated error.\n",
    "    df = pd.concat([df_rel,df_unr])\n",
    "    if save == True:\n",
    "      df.to_csv(PATH / (suffix + language[:3] + '_' + filename), index=False)\n",
    "    df_conj = split_set(PATH, (Bsuffix + language[:3] + '_' + filename), gamma*pi, seed, return_train = False, old = True)\n",
    "    df_conj = pd.concat([df_conj, df_residual])\n",
    "    if save == True:\n",
    "      df_conj.to_csv(PATH / (suffix + language[:3] + '_' + '~' + filename), index=False)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This parameter only activates when the proportion of elements type 'reliable' that the user choosed is equal to the proportion total of element type 'reliable' that exists in the all dataset. This parameter correct the infinite result that return the constant gamma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(!)# filename = 'spa_train.csv' or 'por_train.csv'\n",
    "def return_zetta(num_D_train, pi, alpha, filename):\n",
    "  df_rel = split_label_quality(PATH, Bsuffix + filename, 'reliable', save = False, old = True)\n",
    "  num_rel = len(df_rel)\n",
    "  proportion_rel = num_rel * (1 - pi) / num_D_train\n",
    "  alpha_dec = float(\"{0:.2f}\".format(alpha))\n",
    "  proportion_rel_dec = float(\"{0:.2f}\".format(proportion_rel))\n",
    "  if alpha_dec == proportion_rel_dec:\n",
    "    zetta = 1 / (1 - 1 / num_D_train)\n",
    "  if alpha_dec != proportion_rel_dec:\n",
    "    zetta = 1.0\n",
    "  return zetta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This parameter returns the proportion of elements that used for evaluate the model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vs5KyG_BRVgQ"
   },
   "outputs": [],
   "source": [
    "def return_epsilon(pi, beta):\n",
    "  epsilon = ((1/beta)-1)/((1/pi)-1)\n",
    "  return epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xRZO9B1arOZa"
   },
   "outputs": [],
   "source": [
    "Ksuffix = 'K-spl_test_'\n",
    "# For the total data used in the obtaining model, alpha is the percentaje of the data used to training model, and 1-apha is the data used to evaluated model.\n",
    "#(!)# filename = Esuffix + language[:3] + '_~' + 'train.csv'\n",
    "def split_test(PATH, filename, language, pi, alpha, beta, seed, save = True, old = True, suffix = Ksuffix): #Filename = 'train.csv' or 'test.csv'\n",
    "  #(!)# filename_root = 'spa_~train.csv' or 'por_~train.csv'\n",
    "  filename_root = filename.replace(Esuffix,\"\")\n",
    "  if old == True and (PATH / (suffix + filename_root)).exists():\n",
    "    df = pd.read_csv(PATH / (suffix + filename_root))\n",
    "  if old == False or not(PATH / (suffix + filename)).exists():\n",
    "    epsilon = return_epsilon(pi, beta)\n",
    "    df = pd.read_csv(PATH / (filename))\n",
    "    num_D_train = len(df) + 1\n",
    "    zetta = return_zetta(num_D_train, pi, alpha, filename_root.replace(\"~\",\"\"))\n",
    "    df_residual, df = train_test_split(df, test_size=int(zetta*epsilon*len(df)), random_state=seed, stratify=df.category)#Delete this if existing a error !!!#, stratify=df.category)\n",
    "    if save == True:\n",
    "      df.to_csv(PATH / (suffix + filename_root), index=False)\n",
    "  return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0uQZydbr7iJ2"
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "def normalize_title(title, digits_number, chars_number, test = False):\n",
    "    title = unicodedata.normalize('NFKD', title.lower()).encode('ASCII', 'ignore').decode('utf8')\n",
    "    # replace '--' or '-' or '/' with ''\n",
    "    title = title.replace('-', ' ')\n",
    "    title = title.replace('--', ' ')\n",
    "    # split into tokens by white space\n",
    "    tokens = title.split()\n",
    "    empty = False\n",
    "    if test == True:\n",
    "      if len(tokens) == 0:\n",
    "        empty = True\n",
    "        tokens = [\"<UNK>\", \"<UNK>\", \"<UNK>\"]\n",
    "    \n",
    "    if test == False or test == True and empty == False:\n",
    "      # remove punctuation from each token\n",
    "      table = str.maketrans('', '', string.punctuation)\n",
    "      #The string translate() method returns a string where each character is mapped to its corresponding character in the translation table.\n",
    "      tokens = [w.translate(table) for w in tokens]\n",
    "      # remove remaining tokens that are not alphabetic\n",
    "      if digits_number==0:\n",
    "        tokens = [word for word in tokens if word.isalpha()]\n",
    "      if digits_number>0:\n",
    "        tokens = [word for word in tokens if not(word.isdigit() and len(word)<=digits_number)]\n",
    "      # remove all words that only contains one char.\n",
    "      tokens = [word for word in tokens if len(word)>=chars_number]\n",
    "      # make lower case for each one of our train data.\n",
    "      tokens = [word.lower() for word in tokens]\n",
    "    \n",
    "    # convert into a line\n",
    "    title_text = ' '.join(tokens)\n",
    "    return title_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Kam1UjWsNEh"
   },
   "source": [
    "We split the data in two part, one of parts is to training the model, and the other is to evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I28OPDwH4Evj"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "Fsuffix = 'F-nor_tit_lab_'\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "def normalize_title_label(PATH, filename, digits_number, chars_number, save = True, old = True, test = False, suffix = Fsuffix): #Filename = 'train.csv' or 'test.csv'\n",
    "  if old == True and (PATH / (suffix + filename)).exists():\n",
    "    df = pd.read_csv(PATH / (suffix + filename)) \n",
    "  \n",
    "  if old == False or not((PATH / (suffix + filename)).exists()):\n",
    "    df = pd.read_csv(PATH /  filename) \n",
    "    for i in range(len(df)):\n",
    "      df['title'][i] = normalize_title(df['title'][i], digits_number, chars_number, test = test) # para len_title Pendiente para hallar la cantidad de elementos recomendada en este apartado\n",
    "    if test == False: #Sólo elimina los datos vacíos para el entrenamiento, y para el test, los deja intactos.\n",
    "      df = df[~df.title.isna() & (df.title != 'nan') & (df.title != '')]\n",
    "    if save == True:\n",
    "      df.to_csv(PATH / (suffix + filename), index=False)\n",
    "  return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TayvDzudh-cJ"
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "def trim_title(title, max_len): #This function put the lenght title in a 'max_len' chars.\n",
    "    tokens = title.split()\n",
    "    index_word = len(tokens[0])-1\n",
    "    num_words = 0\n",
    "    while index_word < max_len and num_words < len(tokens):\n",
    "      num_words = num_words + 1\n",
    "      if num_words < len (tokens):\n",
    "        index_word = index_word + len(tokens[num_words]) + 1\n",
    "    if num_words == 0:\n",
    "      tokens = ['']\n",
    "    if num_words > 0:\n",
    "      tokens = tokens[:num_words]\n",
    "    # convert into a line\n",
    "    title_text = ' '.join(tokens)\n",
    "    return title_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are to trim the title to the max lenght chars that used in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ls0DsDNijjWi"
   },
   "outputs": [],
   "source": [
    "FCsuffix = ''\n",
    "pd.options.mode.chained_assignment = None\n",
    "def trim_title_label(PATH, filename, max_len, save = True, suffix = FCsuffix): #Filename = 'train.csv' or 'test.csv'\n",
    "  df = pd.read_csv(PATH /  filename) \n",
    "  for i in range(len(df)):\n",
    "    df['title'][i] = trim_title(df['title'][i], max_len)\n",
    "  df = df[~df.title.isna() & (df.title != 'nan') & (df.title != '')]\n",
    "  if save == True:\n",
    "    df.to_csv(PATH / (suffix + filename), index=False)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LyZEgPeOIwOj"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "Gsuffix = 'G-sor_dat_'\n",
    "def sort_data(PATH, filename, save = True, old = True, suffix = Gsuffix):\n",
    "  if old == True and (PATH / (suffix + filename)).exists():\n",
    "    df = pd.read_csv(PATH / (suffix + filename))\n",
    "  if old == False or not (PATH / (suffix + filename)).exists():\n",
    "    df = pd.read_csv(PATH / filename)\n",
    "    df = df.sort_values(by =['category', 'label_quality', 'title'])\n",
    "    if save == True:\n",
    "      df.to_csv(PATH / (suffix + filename), index=False)\n",
    "  return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z5qxvmDwAnbH"
   },
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from pickle import dump\n",
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from random import randint\n",
    "\n",
    "# save tokens to file, one dialog per line\n",
    "def save_segments(segments, filename):\n",
    "\tdata = '\\n'.join(segments)\n",
    "\tfile = open(filename, 'w')\n",
    "\tfile.write(data)\n",
    "\tfile.close()\n",
    " \n",
    "def load_file(filename):\n",
    "  file=open(filename,'r')\n",
    "  data=file.read()\n",
    "  file.close()\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tP8VX76oYBk6"
   },
   "outputs": [],
   "source": [
    "def return_maxchars(language, save = True, old = True): #Maximum number of chars of the tittle's Mercado Libre\n",
    "  import numpy as np\n",
    "  FAsuffix = 'FA-ret_maxchars_'\n",
    "  if old == True and (PATH / (FAsuffix + language[:3] +'.dat')).exists():\n",
    "    maxchars_model = np.loadtxt(PATH / (FAsuffix + language[:3] +'.dat'))\n",
    "  if old == False or not(PATH / (FAsuffix + language[:3] +'.txt')).exists():\n",
    "    filenames = ['train.csv','~train.csv']\n",
    "    maxchars = []\n",
    "    for step in filenames:\n",
    "      if step == filenames[0]:\n",
    "        filecontent = load_file(PATH / (Fsuffix + Esuffix + language[:3] + '_' + step))\n",
    "      if step == filenames[1]:\n",
    "        filecontent = load_file(PATH / (Fsuffix + Ksuffix + language[:3] + '_' + step))\n",
    "      titles = filecontent.split('\\n')\n",
    "      len_titles = []\n",
    "      for i in range(len(titles)):\n",
    "        len_titles.append(len(titles[i]))\n",
    "      maxchars.append(max(len_titles))\n",
    "    maxchars_model = np.array([max(maxchars)])\n",
    "    if save == True:\n",
    "      np.savetxt(PATH / (FAsuffix + language[:3] +'.dat'), maxchars_model, fmt='%i')\n",
    "  return maxchars_model[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-lp5I1zdXpXj"
   },
   "outputs": [],
   "source": [
    "#The parameter zigma is to variate the lenght of the maxchars in the iterations model, for found the best score.\n",
    "def return_avgchars(language, zigma = 1.0, save = True, old = True): #Mean or average number of chars of the tittle's Mercado Libre\n",
    "  import numpy as np\n",
    "  FBsuffix = 'FB-ret_avgchars_'\n",
    "  if old == True and (PATH / (FBsuffix + language[:3] +'.dat')).exists():\n",
    "    avgchars_model = np.loadtxt(PATH / (FBsuffix + language[:3] +'.dat'))\n",
    "  if old == False or not(PATH / (FBsuffix + language[:3] +'.txt')).exists():\n",
    "    filenames = ['train.csv','~train.csv']\n",
    "    avgchars = []\n",
    "    for step in filenames:\n",
    "      if step == filenames[0]:\n",
    "        filecontent = load_file(PATH / (Fsuffix + Esuffix + language[:3] + '_' + step))\n",
    "      if step == filenames[1]:\n",
    "        filecontent = load_file(PATH / (Fsuffix + Ksuffix + language[:3] + '_' + step))\n",
    "      titles = filecontent.split('\\n')\n",
    "      len_titles = []\n",
    "      for i in range(len(titles)):\n",
    "        len_titles.append(len(titles[i]))\n",
    "      avgchars.append(int(sum(len_titles)/len(len_titles)))\n",
    "    avgchars_model = np.array([max(avgchars)])\n",
    "    if save == True:\n",
    "      np.savetxt(PATH / (FBsuffix + language[:3] +'.dat'), avgchars_model, fmt='%i')\n",
    "  return zigma*avgchars_model[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zHAm7951r6vh"
   },
   "outputs": [],
   "source": [
    "def return_maxdimX(chars_number, maxchars):\n",
    "  #Now we find the maximum number of word that could be inside of the title according the minimum chars_numbers that a word can contains.\n",
    "    maxdimX = int(maxchars/(chars_number+1))\n",
    "    return maxdimX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DeUuax9Rz9PX"
   },
   "outputs": [],
   "source": [
    "Hsuffix = 'H-gen_seg_'\n",
    "filename_root = 'train.csv'\n",
    "#(!)# filename = Fsuffix + Esuffix + language[:3] + '_' + filename_root\n",
    "def generate_segments(PATH, filename, chars_number, maxchars, type_data = 'train', save = True, old = True, suffix = Hsuffix):\n",
    "  segments = list()\n",
    "  if old == True and (PATH / (suffix + filename)).exists():\n",
    "    filecontent = load_file(PATH / (suffix + filename))\n",
    "    segments = filecontent.split('\\n')\n",
    "  if old == False or not(PATH / (suffix + filename)).exists():\n",
    "    df = pd.read_csv(PATH / (filename))\n",
    "    array_titles = array(df['title'])\n",
    "    if type_data == 'train':\n",
    "      array_categories = array(df['category'])\n",
    "    iterations = len(df)\n",
    "    maxdimX = return_maxdimX(chars_number, maxchars)\n",
    "    for i in range(iterations):\n",
    "      titles = str(array_titles[i])\n",
    "      elements = titles.count(' ') + 1\n",
    "      if elements < maxdimX:\n",
    "        for j in range(maxdimX - elements):\n",
    "          titles = titles + ' <NULL>'\n",
    "      if type_data == 'train':\n",
    "        segments.append( titles + ' ' + str(array_categories[i]) )\n",
    "      if type_data == 'test':\n",
    "        segments.append( titles )\n",
    "    if save == True:\n",
    "      save_segments(segments, PATH / (suffix + filename))\n",
    "  return segments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nD5-_XaM1JRv"
   },
   "source": [
    "**Encode Segments**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nTAvtIlrLJ1z"
   },
   "source": [
    "Now, we are setting the word_index to our preference where the special tokens will be first in the dictionary, then the list of categories, and finally the our vocabulary words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A9O4Ps3rUrWM"
   },
   "outputs": [],
   "source": [
    "Isuffix = 'I-gen_tok_'\n",
    "special_tokens_label = ['<NULL>','<UKN>'] #'<NULL>' and '<UKN>' are special tokens reservered to analysis.\n",
    "special_tokens = len(special_tokens_label)\n",
    "#(!) filename = Fsuffix + Esuffix + language[:3] + '_' + 'train.csv',\n",
    "def generate_tokenizer(PATH, filename, num_words, chars_number, maxchars, type_data = 'train', save = True, \n",
    "                       old = True, return_parameter = 'tokenizer, sequences', \n",
    "                       special_tokens_label = special_tokens_label, suffix = Isuffix):\n",
    "  if old == True and (PATH / (Isuffix + filename)).exists():\n",
    "    segments = generate_segments(PATH, filename, chars_number, maxchars, type_data = type_data, save = True, old = True)\n",
    "    tokenizer = load(open(PATH / (Isuffix + filename), 'rb'))\n",
    "    sequences = tokenizer.texts_to_sequences(segments)\n",
    "  if old == False or not(PATH / (Isuffix + filename)).exists():\n",
    "    segments = generate_segments(PATH, filename, chars_number, maxchars, type_data = type_data, save = True, old = True)#False) ID xxx\n",
    "    special_tokens = len (special_tokens_label)\n",
    "    list_categories = return_categories()\n",
    "    number_categories = return_number_categories()\n",
    "    tokenizer = Tokenizer(num_words = special_tokens + number_categories + num_words, filters = '', lower = False, split = ' ', char_level = False,\n",
    "                          oov_token = '<UKN>', document_count = 0)\n",
    "    tokenizer.fit_on_texts(segments)\n",
    "    word_keys = list(tokenizer.word_index)\n",
    "    word_index = {} #Notice that the id: 0 is a id reservered for the tokenizer function.\n",
    "    #word_index['<NULL>'] = 1\n",
    "    #word_index['<UKN>'] = 2\n",
    "    for i in range(special_tokens):\n",
    "      word_index[special_tokens_label[i]] = i + 1\n",
    "    for i in range(number_categories):\n",
    "      word_index[list_categories[i]] = i + 1 + special_tokens\n",
    "    for i in range(len(word_keys)-special_tokens): #num_words):\n",
    "      if word_keys[i+special_tokens] not in word_index:\n",
    "        word_index[word_keys[i+special_tokens]] = i + 1 + special_tokens + number_categories #[i+special_tokens]] = i + 1 + special_tokens + number_categories\n",
    "    tokenizer.word_index = word_index\n",
    "    sequences = tokenizer.texts_to_sequences(segments)\n",
    "    if save == True:\n",
    "      dump(tokenizer, open(PATH / (Jsuffix + filename), 'wb'))  \n",
    "  if return_parameter == 'tokenizer':\n",
    "    return tokenizer\n",
    "  if return_parameter == 'vocabulary_len':\n",
    "    vocabulary_len = len(tokenizer.word_index) + 1\n",
    "    return vocabulary_len\n",
    "  if return_parameter == 'tokenizer, sequences' or return_parameter == 'tokenizer' or return_parameter == 'vocabulary_len':\n",
    "    return tokenizer, sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Yh2M_XYuemO"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "Jsuffix = 'J-gen_seq_'\n",
    "#(!) filename = Fsuffix + Esuffix + 'spa_' + 'train.csv'\n",
    "def generate_sequences(PATH, filename, language, num_words, digits_number, chars_number, type_data = 'train', save = True, old = True,\n",
    "                       return_parameter = 'Xy', suffix = Jsuffix):\n",
    "  list_sequences = []\n",
    "  maxchars = return_avgchars(language) #maxchars = return_maxchars(language)\n",
    "  tokenizer, sequences = generate_tokenizer(PATH, filename, num_words, chars_number, maxchars, type_data = type_data, save = save, old = old, suffix = Isuffix) #save = True, old = True, suffix = Isuffix)\n",
    "  for i in range(len(sequences)):\n",
    "    for j in range(len(sequences[i])):\n",
    "      list_sequences.append(sequences[i][j])\n",
    "  maxdimX = return_maxdimX(chars_number,maxchars)\n",
    "  if type_data == 'test':\n",
    "    array_sequences = np.array(list_sequences).reshape(-1, maxdimX)\n",
    "  if type_data == 'train':\n",
    "    array_sequences = np.array(list_sequences).reshape(-1, maxdimX+1)\n",
    "    if return_parameter == 'X': #Ever returns X vector.\n",
    "      array_sequences = array_sequences[:,:-1]\n",
    "    if return_parameter == 'y': #Ever returns y vector.\n",
    "      array_sequences = array_sequences[:,-1]\n",
    "  if return_parameter == 'vocabulary_len':\n",
    "    vocabulary_len = generate_tokenizer(PATH, filename, num_words, chars_number, maxchars, save = save, old = True, return_parameter = 'vocabulary_len', suffix = Isuffix) #Aquí dejamos old = True para que no vuelva a generar el tokenizer que ya generó en el inicio de este bloque. #save = True, old = True, return_parameter = return_parameter, suffix = Isuffix)\n",
    "    return vocabulary_len\n",
    "  if return_parameter == 'Xy' or return_parameter == 'X' or return_parameter == 'y':\n",
    "    return array_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3nwq7sc62iZp"
   },
   "outputs": [],
   "source": [
    "def return_num_D(filename_root, language):\n",
    "  if (PATH / (Bsuffix + language [:3] + '_' + filename_root)).exists():\n",
    "    df = pd.read_csv(PATH / (Bsuffix + language [:3] + '_' + filename_root))\n",
    "  if not (PATH / (Bsuffix + language [:3] + '_' + filename_root)).exists():\n",
    "    df = split_language(PATH, filename_root, language, old = True)\n",
    "  num_D = len(df)\n",
    "  return num_D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_A8xBuM9hWl7"
   },
   "outputs": [],
   "source": [
    "#(!)# filename_root = 'train.csv'\n",
    "#filename_root = 'train.csv'\n",
    "def generate_data_model(PATH, filename_root, language, psi, beta, alpha, zigma, chars_number, digits_number, seed, save = True, old = True):\n",
    "  if old == True and (PATH / (Fsuffix + Esuffix + language[:3] + '_' + filename_root)).exists() and (PATH / (Fsuffix + Ksuffix + language[:3] + '_' + filename_root)).exists():\n",
    "    print(\"\\t\\t\\t\\tgenerate_data_model() previously executed\")\n",
    "  if old == False or not (PATH / (Fsuffix + Esuffix + language[:3] + '_' + filename_root)).exists() or not (PATH / (Fsuffix + Ksuffix + language[:3] + '_' + filename_root)).exists():\n",
    "    pi = psi * beta\n",
    "    \n",
    "    df = split_data(PATH, filename_root, language, pi, alpha, seed, save = save, old = old)\n",
    "    print(\"\\t\\t\\t\\tsplit_data() finished\\n\",df.head(n = 5))\n",
    "    df = normalize_title_label(PATH , Esuffix + language[:3] + '_' + filename_root, digits_number, chars_number, save = save, old = save)\n",
    "    print(\"\\t\\t\\t\\tnormalize_title_label() data finished\\n\",df.head(n = 5))\n",
    "    df = split_test(PATH, Esuffix + language[:3] + '_~' + filename_root, language,  pi, alpha, beta, seed, save = save, old = old) \n",
    "    print(\"\\t\\t\\t\\tsplit_test() finished\\n\",df.head(n = 5))\n",
    "    df = normalize_title_label(PATH , Ksuffix + language[:3] + '_~' + filename_root, digits_number, chars_number, save = save, old = old) \n",
    "    print(\"\\t\\t\\t\\tnormalize_title_label() test finished\\n\",df.head(n = 5))\n",
    "\n",
    "    avgchars = return_avgchars(language, zigma = zigma, save = True, old = False) #Mean or average number of chars of the tittle's Mercado Libre\n",
    "    df = trim_title_label(PATH, Fsuffix + Esuffix + language[:3] + '_' + filename_root, max_len = avgchars, save = True) #Filename = Fsuffix + Esuffix + language[:3] + '_' + filename_root or Fsuffix + Ksuffix + language[:3] + '_~' + filename_root\n",
    "    print(\"\\t\\t\\t\\ttrim_title_label() data finished\\n\",df.head(n = 5))\n",
    "    df = trim_title_label(PATH, Fsuffix + Ksuffix + language[:3] + '_~' + filename_root, max_len = avgchars, save = True)\n",
    "    print(\"\\t\\t\\t\\ttrim_title_label() test finished\\n\",df.head(n = 5))\n",
    "    \n",
    "    #This line was deleted because it isn't necessary for the trainning model\n",
    "    #df = sort_data(PATH, Fsuffix + Esuffix + language[:3] + '_' + filename_root, save = save, old = old)\n",
    "    #print(\"\\t\\t\\t\\tsort_data() finished\\n\",df.head(n = 5))\n",
    "    # It's not necessary sort the test train data '~train.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fPhGWepDq5tK"
   },
   "outputs": [],
   "source": [
    "#(!)#  filename_root = 'train.csv'\n",
    "def return_sequences(PATH, filename_root, language, num_words, digits_number, chars_number, return_parameter = 'train', save = True, old = True):\n",
    "  #Trainning part\n",
    "  if return_parameter == 'train':\n",
    "    #If the accuracy reduces, then do execute this line.\n",
    "    #array_sequences = generate_sequences(PATH, Gsuffix + Fsuffix + Esuffix + language[:3] + '_' + filename_root, language, num_words, digits_number, chars_number, type_data = 'train', return_parameter = 'Xy', save = save, old = old)\n",
    "    array_sequences = generate_sequences(PATH, Fsuffix + Esuffix + language[:3] + '_' + filename_root, language, \n",
    "                                         num_words, digits_number, chars_number, type_data = 'train',\n",
    "                                         return_parameter = 'Xy', save = save, old = old)\n",
    "  #Test part\n",
    "  if return_parameter == 'test':\n",
    "    array_sequences = generate_sequences(PATH, Fsuffix + Ksuffix + language[:3] + '_~' + filename_root, language,\n",
    "                                         num_words, digits_number, chars_number, type_data = 'test',\n",
    "                                         return_parameter = 'Xy', save = save, old = old)\n",
    "  if return_parameter == 'train':\n",
    "    X, y = array_sequences[:,:-1], array_sequences[:,-1]\n",
    "    return X, y\n",
    "  if return_parameter == 'test':\n",
    "    X = array_sequences\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kWcjfCdJmYxT"
   },
   "source": [
    "**Obtaining the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ilqxTcSAT8XL"
   },
   "outputs": [],
   "source": [
    "#(!)# filename_root = 'train.csv'\n",
    "\n",
    "kappa = 2\n",
    "hidden_size = 50\n",
    "activation = ['relu', 'softmax']\n",
    "loss = 'categorical_crossentropy'\n",
    "optimizer = 'adam'\n",
    "metrics = ['accuracy']\n",
    "batch_size = 128\n",
    "epochs = 100\n",
    "\n",
    "def generate_model(PATH, filename_root, language, num_words, digits_number, \n",
    "                   chars_number, kappa = kappa, hidden_size = hidden_size, activation = activation, loss = loss, optimizer = optimizer, \n",
    "                   metrics = metrics, batch_size = batch_size, epochs = epochs, old = True):\n",
    "  \n",
    "  if (PATH / ('model_' + language[:3] + '_' + filename_root[:-4] + '.h5')).exists() and old == True:\n",
    "    print(\"\\t\\t\\t\\t\\t\\t\\t\\tgenerate_model() previously executed\")\n",
    "\n",
    "  if not(PATH / ('model_' + language[:3] + '_' + filename_root[:-4] + '.h5')).exists() or old == False:\n",
    "    X, y = return_sequences(PATH, filename_root, language, num_words, digits_number, chars_number, \n",
    "                            return_parameter = 'train', save = True, old = True)\n",
    "    \n",
    "    number_categories = return_number_categories()\n",
    "    vocabulary_len = special_tokens + number_categories + num_words + 1\n",
    "    y = to_categorical(y, num_classes = vocabulary_len)\n",
    "    seq_length = X.shape[1]\n",
    "\n",
    "\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocabulary_len, hidden_size, input_length = seq_length))\n",
    "    model.add(LSTM(int(kappa*hidden_size), return_sequences = True))\n",
    "    model.add(LSTM(int(kappa*hidden_size)))\n",
    "    model.add(Dense(int(kappa*hidden_size), activation = activation[0]))\n",
    "    model.add(Dense(vocabulary_len, activation = activation[1]))\n",
    "    print(model.summary())\n",
    "\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss = loss, optimizer = optimizer, metrics = metrics)\n",
    "    # fit model\n",
    "    model.fit(X, y, batch_size = batch_size, epochs = epochs)\n",
    "\n",
    "    # save the model\n",
    "    model.save(PATH / ('model_' + language[:3] + '_' + filename_root[:-4] + '.h5'))\n",
    "    print(\"\\t\\t\\t\\t\\t\\t\\t\\tgenerate_model() executed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are creating the functions that return a range of number for each our parameter in the iterations of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-VAAySxeR10P"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def F(n): #Fibonacci function\n",
    "    if n == 0: return 0\n",
    "    elif n == 1: return 1\n",
    "    else: return F(n-1)+F(n-2)\n",
    "\n",
    "def fj(j): #Proportion Fibonacci\n",
    "  fj = F(j+2)/F(j+3)\n",
    "  return fj\n",
    "\n",
    "def return_fibo(a, b, N, return_int = False): #Return a range in fibo numbers.\n",
    "  Bj = np.zeros(N)\n",
    "  if return_int == True:\n",
    "    Bj[0] = int(b)\n",
    "    Bj[N-1] = int(a)\n",
    "  if return_int == False:\n",
    "    Bj[0] = b\n",
    "    Bj[N-1] = a\n",
    "  n = N-2\n",
    "  bj = b\n",
    "  for i in range(n):\n",
    "    bj = a + (bj - a)*(1 - fj(i))\n",
    "    if return_int == True:\n",
    "      bj = int(bj)\n",
    "    Bj[i+1] = bj\n",
    "  Bi = np.zeros(N)\n",
    "  for i in range(N):\n",
    "    Bi[i] = Bj[N-1-i]\n",
    "  if N == 1:\n",
    "    Bi[0] = a\n",
    "  Bi = np.unique(Bi)\n",
    "  if b - a < 0:\n",
    "    Bi = np.flip(Bi)\n",
    "  return Bi\n",
    "\n",
    "def return_range(a, b, N, return_int = False): #Return a range in uniform distribucion\n",
    "  if return_int == True:\n",
    "    range_parameter = np.arange(int(a),int(b),(b-a)/(N-1))\n",
    "  range_parameter = np.arange(a,b,(b-a)/(N-1))\n",
    "  range_parameters = np.zeros(N)\n",
    "  for i in range(N-1):    \n",
    "    range_parameters[i] = range_parameter[i]\n",
    "  range_parameters[-1] = b\n",
    "  if return_int == True:\n",
    "    for i in range(len(range_parameters)):\n",
    "      range_parameters[i] = int(range_parameters[i])\n",
    "  if len(range_parameters) == 2:\n",
    "    range_parameters[1] = b\n",
    "  range_parameters = np.unique(range_parameters)\n",
    "  if b - a < 0:\n",
    "    range_parameters = np.flip(range_parameters)\n",
    "  return range_parameters\n",
    "\n",
    "def return_range_parameter(value, range_value, return_int = False): #Returns a range based to a value recomended and if relative proportions.\n",
    "  steps = range_value[2]\n",
    "  range_parameter = np.arange(int(value*range_value[0]),int(value*range_value[1]),int(value*(range_value[1]-range_value[0])/(steps-1)))\n",
    "  range_parameters = np.zeros(steps)\n",
    "  for i in range(steps-1):    \n",
    "    range_parameters[i] = range_parameter[i]\n",
    "  range_parameters[-1] = int(value*range_value[1])\n",
    "  if return_int == True:\n",
    "    for i in range(len(range_parameters)):\n",
    "      range_parameters[i] = int(range_parameters[i])\n",
    "  if len(range_parameters) == 2:\n",
    "    range_parameters[1] = value*range_value[1]\n",
    "  return np.unique(range_parameters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Obtaining the Best Accuracy Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YQ2WHN6iyZYX"
   },
   "outputs": [],
   "source": [
    "# THIS AREA HELPS US TO FIND THE BEST PARAMETER THAT RETURNS THE BEST BACC ACCURACY\n",
    "import time\n",
    "def predict_sequences(PATH, filename_root, language, num_words, digits_number, chars_number, return_y = False, return_parameter = 'train'):\n",
    "  if return_parameter == 'train':\n",
    "    X, y = return_sequences(PATH, filename_root, language, num_words, digits_number, chars_number, return_parameter = 'train', save = True, old = True)\n",
    "  if return_parameter == 'test':\n",
    "    X = return_sequences(PATH, filename_root, language, num_words, digits_number, chars_number, return_parameter = 'test', save = True, old = True)\n",
    "  model = load_model(PATH / ('model_' + language[:3] + '_' + filename_root[:-4] + '.h5'))\n",
    "  yhat = model.predict_classes(X, verbose=0)\n",
    "  if return_y == False:\n",
    "    return yhat\n",
    "  if return_y == True:\n",
    "    return y, yhat\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "def return_accuracy(y, yhat, type = 'BACC'):\n",
    "  if type == 'BACC':\n",
    "    accuracy = balanced_accuracy_score(y, yhat)\n",
    "  return accuracy\n",
    "\n",
    "#The suffix to name the file that contains the information of each model iterated in the previous cell code.\n",
    "Osuffix = 'models_accuracy_'\n",
    "filename_root = 'train.csv'\n",
    "languages = ['portuguese', 'spanish']\n",
    "seed = 8 # Seed used to the trainning model.\n",
    "psi = 1/200 # Proportion of the total data data used in the analysis of the model and for each language.\n",
    "beta = 0.30 # Proportion of the all data used in this model, where 1-beta is the proportion the data to evaluate model.\n",
    "\n",
    "range_alpha = return_fibo(0.25,0.3,1) #Proportion of elements type 'reliable' that the user want to choose in the model.\n",
    "range_zigma = return_fibo(0.75,1.5,1) #Proportion of the average chars lenght title's Mercado Libre, where zigma = 1.0 generate that the max chars lenght used in the model is equal to the average chars in the data.\n",
    "range_chars_number = return_fibo(5,8,1, return_int = True) #The minimun chars that a word can contains in the learning model.\n",
    "range_digits_number = return_fibo(0,8,1, return_int = True) #The minimun digits that a word can contains in the learning model.\n",
    "\n",
    "range_num_words = return_fibo(13000,21000,1, return_int = True) #The maximum number of words that the model learned.\n",
    "range_kappa = return_fibo(1.5,3,1) #The proportion of the total hidden shapes in the neural network\n",
    "range_hidden_size = return_fibo(125,150,1, return_int = True) \n",
    "range_batch_size = return_fibo(64, 128, 1, return_int = True)\n",
    "range_epochs = return_fibo(250,300,1, return_int = True)\n",
    "\n",
    "print(\"<--------------------------------------MODELS PARAMETERS-------------------------------------->\")\n",
    "print(\"FILENAME OUT:\", Osuffix + filename_root)\n",
    "print(\"LANGUAGES:\",languages)\n",
    "print(\"SEED:\",seed)\n",
    "print(\"PSI:\",psi)\n",
    "print(\"BETA:\",beta)\n",
    "for language in languages:\n",
    "  print(\"LANGUAGE:\", language)\n",
    "  for alpha in range_alpha:\n",
    "    alpha = float(alpha)\n",
    "    print(\"\\tALPHA:\",alpha)\n",
    "    for zigma in range_zigma:\n",
    "      zigma = float(zigma)\n",
    "      print(\"\\t\\tZIGMA:\",zigma)\n",
    "      for chars_number in range_chars_number:\n",
    "        chars_number = int(chars_number)\n",
    "        print(\"\\t\\t\\tCHARS_NUMBER:\",chars_number)\n",
    "        for digits_number in range_digits_number:\n",
    "          digits_number = int(digits_number)\n",
    "          print(\"\\t\\t\\t\\tDIGITS_NUMBER:\",digits_number)\n",
    "          repeat_data = 0\n",
    "          repeat_model = 0\n",
    "          iterbreak = False\n",
    "          if (PATH / (Osuffix + language[:3] + '_' + filename_root)).exists():\n",
    "            df = pd.read_csv(PATH / (Osuffix + language[:3] + '_' + filename_root))\n",
    "            df_data = df.loc[(df['seed'] == seed) & (df['psi'] == psi) & (df['beta'] == beta) & (df['alpha'] == alpha) & (df['zigma'] == zigma) & (df['chars_number'] == chars_number) & (df['digits_number'] == digits_number)] #This line generates erros, I don't know the reason & (df['digits_number'] == digits_number)\n",
    "            repeat_data = len(df_data)\n",
    "            if repeat_data > 0:\n",
    "              for i in range(len(range_num_words)):\n",
    "                for h in range(len(range_kappa)):\n",
    "                  for j in range(len(range_hidden_size)):\n",
    "                    for k in range(len(range_batch_size)):\n",
    "                      for l in range(len(range_epochs)):\n",
    "                        df_model = df.loc[(df['seed'] == seed) & (df['psi'] == psi) & (df['beta'] == beta) & (df['alpha'] == alpha) & (df['zigma'] == zigma) & (df['chars_number'] == chars_number) & (df['digits_number'] == digits_number) & (df['num_words'] == int(range_num_words[i])) & (df['kappa'] == float(range_kappa[h])) & (df['hidden_size'] == int(range_hidden_size[j])) & (df['batch_size'] == int(range_batch_size[k])) & (df['epochs'] == int(range_epochs[l]))]\n",
    "                        repeat_model += len(df_model)\n",
    "                        if (len(df_model) == 0):\n",
    "                          iterbreak = True\n",
    "                          break\n",
    "                      if iterbreak == True:\n",
    "                        break\n",
    "                    if iterbreak == True:\n",
    "                      break\n",
    "                  if iterbreak == True:\n",
    "                    break\n",
    "                if iterbreak == True:\n",
    "                  break\n",
    "          if iterbreak == False and repeat_model > 0:\n",
    "            print(\"\\t\\t\\t\\t\\tgenerate_data_model() and generate_model() previously executed\")\n",
    "          if iterbreak == True or iterbreak == False and repeat_model == 0:\n",
    "            clocktime_data = time.time()\n",
    "            generate_data_model(PATH, 'train.csv', language, psi, beta, alpha, zigma, chars_number, digits_number, seed, save = True, old = False)\n",
    "            time_data = time.time() - clocktime_data\n",
    "            activation = ['relu', 'softmax']\n",
    "            activation0 = activation[0]\n",
    "            activation1 = activation[1]\n",
    "            loss = 'categorical_crossentropy'\n",
    "            optimizer = 'adam'\n",
    "            metrics = ['accuracy']\n",
    "            for num_words in range_num_words:\n",
    "              num_words = int(num_words)\n",
    "              print(\"\\t\\t\\t\\t\\tNUM_WORDS:\",num_words)\n",
    "              for kappa in range_kappa:\n",
    "                kappa = float(kappa)\n",
    "                print(\"\\t\\t\\t\\t\\tKAPPA:\",kappa)\n",
    "                for hidden_size in range_hidden_size:\n",
    "                  hidden_size = int(hidden_size)\n",
    "                  print(\"\\t\\t\\t\\t\\t\\tHIDDEN_SIZE:\",hidden_size)\n",
    "                  for batch_size in range_batch_size:\n",
    "                    batch_size = int(batch_size)\n",
    "                    print(\"\\t\\t\\t\\t\\t\\t\\tBATCH_SIZE:\", batch_size)\n",
    "                    for epochs in range_epochs:\n",
    "                      epochs = int(epochs)\n",
    "                      print(\"\\t\\t\\t\\t\\t\\t\\t\\tEPOCHS:\", epochs)\n",
    "                      repeat_model = 0\n",
    "                      if (PATH / (Osuffix + language[:3] + '_' + filename_root)).exists():\n",
    "                        df = pd.read_csv(PATH / (Osuffix + language[:3] + '_' + filename_root))\n",
    "                        df_model = df.loc[(df['seed'] == seed) & (df['psi'] == psi) & (df['beta'] == beta) & (df['alpha'] == alpha) & (df['zigma'] == zigma) & (df['chars_number'] == chars_number) & (df['digits_number'] == digits_number) & (df['num_words'] == num_words) & (df['kappa'] == kappa) & (df['hidden_size'] == hidden_size) & (df['batch_size'] == batch_size) & (df['epochs'] == epochs)]\n",
    "                        repeat_model = len(df_model)\n",
    "                      if (repeat_data > 0 and repeat_model > 0):\n",
    "                        print(\"\\t\\t\\t\\t\\t\\t\\t\\t\\t generate_model() previously executed\")\n",
    "                      \n",
    "                      if not (repeat_data > 0 and repeat_model > 0):\n",
    "                        clocktime_model = time.time()\n",
    "                        generate_model(PATH, filename_root, language, num_words, digits_number, chars_number, kappa = kappa, hidden_size = hidden_size, activation = activation, loss = loss, optimizer = optimizer, metrics = metrics, batch_size = batch_size, epochs = epochs, old = False)\n",
    "                        \n",
    "                        time_model = time.time() - clocktime_model\n",
    "                        \n",
    "                        y, yhat = predict_sequences(PATH, filename_root, language, num_words, digits_number, chars_number, return_y = True, return_parameter = 'train')\n",
    "                        \n",
    "                        accuracy = return_accuracy(y, yhat, type = 'BACC')\n",
    "                        \n",
    "                        dictionary_model_parameter = {\"accuracy\":[accuracy], \"time\":[(time_data + time_model) / 60], \"seed\":[seed], \"psi\":[psi], \n",
    "                                                      \"beta\":[beta], \"alpha\":[alpha], \"zigma\":[zigma], \"chars_number\":[chars_number], \n",
    "                                                      \"digits_number\":[digits_number], \"num_words\":[num_words], \"kappa\":[kappa], \"hidden_size\":[hidden_size], \n",
    "                                                      \"activation0\":[activation0], \"activation1\":[activation1], \"loss\":[loss], \"optimizer\":[optimizer], \"metrics\":[metrics[0]], \n",
    "                                                      \"batch_size\":[batch_size], \"epochs\":[epochs]}\n",
    "                        \n",
    "                        model_parameter = pd.DataFrame(dictionary_model_parameter) \n",
    "\n",
    "                        if (PATH / (Osuffix + language[:3] + '_' + filename_root)).exists():\n",
    "                          models_parameters = pd.read_csv(PATH / (Osuffix + language[:3] + '_' + filename_root))\n",
    "                          models_parameters = pd.concat([models_parameters, model_parameter])\n",
    "                          models_parameters.to_csv(PATH / (Osuffix + language[:3] + '_' + filename_root), index = False)\n",
    "\n",
    "                        if not(PATH / (Osuffix + language[:3] + '_' + filename_root)).exists():\n",
    "                          model_parameter.to_csv(PATH / (Osuffix + language[:3] + '_' + filename_root), index = False)\n",
    "\n",
    "                        print(model_parameter.head(n = 1))\n",
    "                    \n",
    "print(\"<----------------------------------MODELS PARAMETERS FINISHED--------------------------------->\")  \n",
    "for language in languages:\n",
    "  models_parameters = pd.read_csv(PATH / (Osuffix + language[:3] + '_' + filename_root))\n",
    "  models_parameters = models_parameters.sort_values(by = ['accuracy', 'time'], ascending = [False, True])\n",
    "  print(\"\\nlanguage:\",language)\n",
    "  print(models_parameters.head(n = len(models_parameters)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jy3l3O1IQC7h"
   },
   "outputs": [],
   "source": [
    "# Now we are going to create the best model based in the best accuracy that we found in the model.\n",
    "filename_root = 'train.csv'\n",
    "languages = ['portuguese', 'spanish']\n",
    "\n",
    "print(\"<----------------------------------BEST PARAMETERS MODEL FOUNDED----------------------------------->\")\n",
    "for language in languages:\n",
    "    len_models_parameters = 0\n",
    "    if (PATH / (Osuffix + language[:3] + '_' + filename_root)).exists:\n",
    "      models_parameters = pd.read_csv(PATH / (Osuffix + language[:3] + '_' + filename_root))\n",
    "      len_models_parameters = len(models_parameters)\n",
    "    if len_models_parameters > 0:\n",
    "      if len_models_parameters > 1:\n",
    "        models_parameters = models_parameters.sort_values(by = ['psi', 'accuracy', 'time'], ascending = [False, False, True])\n",
    "      seed = models_parameters['seed'][0] #5 # Seed used to the trainning model.\n",
    "      psi = models_parameters['psi'][0] #1/100 # Proportion of the total data data used in the analysis of the model.\n",
    "      beta = models_parameters['beta'][0] #0.80 # Proportion of the all data used in this model, where 1-beta is the proportion the data to evaluate model.\n",
    "      alpha = models_parameters['alpha'][0] #2*0.04909269508679993 # Proportion of the total data that will use with label : 'reliable' for the training model. \n",
    "      zigma = models_parameters['zigma'][0] #1.0 Proportion of the average of total maxchars that you want use in your model. Eg. zigma = 1.0 -> maxchars = avgchars (~67chars)\n",
    "      chars_number = models_parameters['chars_number'][0] #5 #Only it will show the words that are a minimum length of 2 chars.\n",
    "      digits_number = models_parameters['digits_number'][0] #0 #Only it will show the numbers that are greater than 'digits_number'\n",
    "      num_words = models_parameters['num_words'][0] #10000 #Set the number of words in your vocabulary. If num_words=='none', this will not have limits in its length.\n",
    "      kappa = models_parameters['kappa'][0] #50\n",
    "      hidden_size = models_parameters['hidden_size'][0] #50\n",
    "      activation0 = models_parameters['activation0'][0] #'relu'\n",
    "      activation1 = models_parameters['activation1'][0] #'softmax'\n",
    "      activation = [activation0, activation1] #['relu', 'softmax']\n",
    "      loss = models_parameters['loss'][0] #'categorical_crossentropy'\n",
    "      optimizer = models_parameters['optimizer'][0] #'adam'\n",
    "      metrics = [models_parameters['metrics'][0]] #['accuracy']\n",
    "      batch_size = models_parameters['batch_size'][0] #128\n",
    "      epochs = models_parameters['epochs'][0] #100\n",
    "      print(\"language:\",language)\n",
    "      print(models_parameters.head(n = 1))\n",
    "      generate_data_model(PATH, filename_root, language = language, psi = psi, beta = beta, alpha = alpha, zigma = zigma, chars_number = chars_number, digits_number = digits_number, seed = seed, save = True, old = False)\n",
    "      generate_model(PATH, filename_root, language = language, num_words = num_words, digits_number = digits_number, chars_number = chars_number, kappa = kappa, hidden_size = hidden_size, activation = activation, loss = loss, optimizer = optimizer, metrics = metrics, batch_size = batch_size, epochs = epochs, old = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fpr7D83DXpU9"
   },
   "outputs": [],
   "source": [
    "# Función para separar un archivo final tipo 'test.csv' por lenguaje y poder indexarlo\n",
    "Lsuffix = 'L-tra_lan_spl_'\n",
    "filename_test_root = 'test.csv'\n",
    "languages = ['portuguese', 'spanish']\n",
    "def transf_language_split(PATH = PATH, filename_test_root = filename_test_root , languages = languages, return_dataframe = languages[0], save = True, old = True, Lsuffix = Lsuffix):\n",
    "  for language in languages:\n",
    "    suffix = Lsuffix + language[:3] + '_'\n",
    "    if old == True and (PATH / (suffix + filename_test_root)).exists():\n",
    "        df = pd.read_csv(PATH / (suffix + filename_test_root))\n",
    "    if old == False or not(PATH / (suffix + filename_test_root)).exists():\n",
    "      df = pd.read_csv(PATH / filename_test_root)\n",
    "      df = df[df['language'] == language]\n",
    "      if save == True:\n",
    "        df.to_csv(PATH / (suffix + filename_test_root), index=False)\n",
    "  if return_dataframe != '':\n",
    "    language = return_dataframe\n",
    "    df = pd.read_csv(PATH / (Lsuffix + language[:3] + '_' + filename_test_root))\n",
    "    return df\n",
    "  if return_dataframe == '':\n",
    "    print(\"trans_language_split() executed\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W2Lfv2jiDrtz"
   },
   "outputs": [],
   "source": [
    "#separando data evaluativa en el primer idioma\n",
    "filename_test_root = 'test.csv'\n",
    "languages = ['portuguese', 'spanish']\n",
    "df = transf_language_split(PATH = PATH, filename_test_root = filename_test_root, languages = languages, return_dataframe = languages[0], save = True,\n",
    "                           old = False)\n",
    "df.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6EvVp10MEMVb"
   },
   "outputs": [],
   "source": [
    "#separando data evaluativa en el segundo idioma\n",
    "df = transf_language_split(PATH = PATH, filename_test_root = filename_test_root, languages = languages, return_dataframe = languages[1], save = True,\n",
    "                           old = True)\n",
    "df.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rrbP5X-QFjBG"
   },
   "outputs": [],
   "source": [
    "#(!)# ID E0 Área para convertir vector predicción en categorías\n",
    "def generate_categories(yhat, special_tokens = special_tokens):\n",
    "  list_categories = return_categories()\n",
    "  y_hat_categories = []\n",
    "  for i in range(len(yhat)):\n",
    "    y_hat_categories.append(list_categories[yhat[i] - special_tokens -1])\n",
    "  return y_hat_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BQvA0F2X0EhI"
   },
   "outputs": [],
   "source": [
    "#(!)# filename_test in = Nsuffix + Lsuffix + language[:3] + '_' + filename_test_root\n",
    "#(!)# filename_test out = Msuffix + Nsuffix + Lsuffix + language[:3] + '_' + filename_test_root\n",
    "Msuffix = 'M-tra_lan_con_'\n",
    "filename_test = filename_test_root\n",
    "languages = ['portuguese', 'spanish']\n",
    "def transf_language_concat(PATH = PATH, filename_test = filename_test, languages = languages, return_dataframe = True, save = True, old = True, suffix = Msuffix): \n",
    "  if old == True and (PATH / (suffix + filename_test)).exists():\n",
    "      df = pd.read_csv(PATH / (suffix + filename_test))\n",
    "  if old == False or not(PATH / (suffix + filename_test)).exists():\n",
    "    df = pd.read_csv(PATH / (Nsuffix + Fsuffix + Lsuffix + languages[0][:3] + '_' + filename_test))\n",
    "    for i in range(1,len(languages)):\n",
    "      language = languages[i]\n",
    "      df_lang = pd.read_csv(PATH / (Nsuffix + Fsuffix + Lsuffix + language[:3] + '_' + filename_test))\n",
    "      df = pd.concat([df, df_lang])\n",
    "    df = df.sort_values(by =['id']) #or by='id'\n",
    "    if save == True:\n",
    "      df.to_csv(PATH / (suffix + filename_test), index=False)\n",
    "  if return_dataframe == False:\n",
    "    print(\"transf_language_concat() executed\")\n",
    "  if return_dataframe == True:\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "FHnAhLcDLWE-"
   },
   "outputs": [],
   "source": [
    "#(!)# ID F0\n",
    "#(!)# filename in = Lsuffix + language[:3] + '_' + 'test.csv'\n",
    "def predict_test(PATH, filename, language, num_words, digits_number, chars_number):\n",
    "  df = normalize_title_label(PATH, filename, digits_number, chars_number, save = True, old = True, test = True)\n",
    "  avgchars = return_avgchars(language)\n",
    "  df = trim_title_label(PATH, filename, max_len = avgchars, save = True)\n",
    "  X = generate_sequences(PATH, Fsuffix + filename, language, num_words, digits_number, chars_number, type_data = 'test', save = True, old = True)\n",
    "  model = load_model(PATH / ('model_' + language[:3] + '_' + 'train.h5'))\n",
    "  yhat = model.predict_classes(X, verbose=0)\n",
    "  return yhat\n",
    "\n",
    "filename_test = 'test.csv'\n",
    "languages = ['portuguese', 'spanish']\n",
    "def predict_test_lang(PATH, filename_test, languages, num_words, digits_number, chars_number):\n",
    "  y_hat_categories_lan = list()\n",
    "  for language in languages:\n",
    "    yhat = predict_test(PATH, Lsuffix + language[:3] + '_' + filename_test, language, num_words, digits_number, chars_number)\n",
    "    y_hat_categories = generate_categories(yhat)\n",
    "    y_hat_categories_lan.append(y_hat_categories) \n",
    "  return y_hat_categories_lan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are return a matrix with each predictions for each language that contains the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ax87PpARqLw1"
   },
   "outputs": [],
   "source": [
    "filename_test = 'test.csv'\n",
    "languages = ['portuguese', 'spanish']\n",
    "y_hat_categories_lan = predict_test_lang(PATH, filename_test, languages, num_words, digits_number, chars_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6zutoZVQhcSN"
   },
   "outputs": [],
   "source": [
    "#(!)# filename_test in = Lsuffix + language[:3] + '_' + 'test.csv'\n",
    "#(!)# filename_test out = Nsuffix + Lsuffix + language[:3] + '_' + 'test.csv'\n",
    "filename_test = 'test.csv'\n",
    "columns = 'id'\n",
    "Nsuffix = 'att_col_'\n",
    "def attach_column(y_hat_categories, PATH = PATH, filename_test = filename_test, columns = columns, \n",
    "                  save = True, old = True, suffix = Nsuffix):\n",
    "  if old == True and (PATH / (suffix + filename_test)).exists():\n",
    "    df = pd.read_csv(PATH / filename_test)\n",
    "  if old == False or not (PATH / (suffix + filename_test)).exists():\n",
    "    label_attach = 'category'\n",
    "    df = pd.read_csv(PATH / filename_test)\n",
    "    df = df[[columns]]\n",
    "    df = df.iloc[:,0:1]\n",
    "    df[label_attach] = y_hat_categories\n",
    "    if save == True:\n",
    "      df_copy = df[['id','category']]\n",
    "      df_copy = df.iloc[:,0:2]\n",
    "      df_copy.to_csv(PATH / (suffix + filename_test), index=False)\n",
    "      return df_copy\n",
    "  return df\n",
    "\n",
    "#(!)# filename_test_root in = 'test.csv'\n",
    "filename_test_root = 'test.csv'\n",
    "columns = 'id'\n",
    "languages = ['spanish','portuguese']\n",
    "# This part attach the columns with the results previously generated.\n",
    "def attach_column_lang(y_hat_categories_lan, PATH = PATH, filename_test_root = filename_test_root, columns = columns, languages = languages, \n",
    "                       return_dataframe = languages[0], save = True, old = True):\n",
    "  for i in range(len(languages)):\n",
    "    language = languages[i]\n",
    "    filename_test = Fsuffix + Lsuffix + language[:3] + '_' + filename_test_root\n",
    "    df = attach_column(y_hat_categories_lan[i], PATH = PATH, filename_test = filename_test, columns = columns, save = save, old = old)\n",
    "  if return_dataframe == '':\n",
    "    print(\"attach_column_lang() executed\")\n",
    "  if return_dataframe in languages:\n",
    "    language = return_dataframe\n",
    "    df = pd.read_csv(PATH / (Nsuffix + Fsuffix + Lsuffix + language[:3] + '_' + filename_test_root))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eCCMVQJ-waj3"
   },
   "outputs": [],
   "source": [
    "filename_test_root = 'test.csv'\n",
    "columns = 'id'\n",
    "languages = ['portuguese', 'spanish']\n",
    "# Attaching the spanish column\n",
    "df_spa = attach_column_lang(y_hat_categories_lan, PATH, filename_test_root, columns, languages, \n",
    "                        return_dataframe = languages[0], save = True, old = True)\n",
    "df_spa.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pPBHBo7gxKwB"
   },
   "outputs": [],
   "source": [
    "# Attaching the portuguese column\n",
    "df_por = attach_column_lang(y_hat_categories_lan, PATH, filename_test_root, columns, languages, \n",
    "                        return_dataframe = languages[1], save = True, old = True)\n",
    "df_por.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we generate the submission with contains all data predict and ordered with its original id. This a best way to used our limited processing recourses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TaOwEF84EvDF"
   },
   "outputs": [],
   "source": [
    "#Now we concat all data of both languages.\n",
    "filename_test_root = 'test.csv'\n",
    "df = transf_language_concat(PATH = PATH, filename_test = filename_test_root, languages = languages, \n",
    "                            return_dataframe = True, save = True, old = True) \n",
    "df.head(25)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "MeLiDataChallenge2019_Project.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
